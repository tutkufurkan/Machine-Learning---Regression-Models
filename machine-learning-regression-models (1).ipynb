{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13456049,"sourceType":"datasetVersion","datasetId":8541398},{"sourceId":13456063,"sourceType":"datasetVersion","datasetId":8541408},{"sourceId":13458224,"sourceType":"datasetVersion","datasetId":8542807},{"sourceId":13497815,"sourceType":"datasetVersion","datasetId":8570078},{"sourceId":13499763,"sourceType":"datasetVersion","datasetId":8571434}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INSTRUCTION\n1. [Linear Regression](#1)\n2. [Multiple Linear Regression](#2)\n3. [Polynomial Lineer Regression](#3)\n4. [Decision Tree Regression](#4)\n5. [Random Forest Regression](#5)\n6. [Evaluation Regression Models](#6)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# data visualization\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nimport seaborn as sns\n# machine learning\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<a id = \"1\"></a>\n## Linear Regression","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import data\nlinear_regression = pd.read_csv(\"/kaggle/input/linear-regression-dataset/linear_regression_dataset.csv\", sep = \";\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"linear_regression.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"linear_regression.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot data\nplt.figure(figsize=(10, 6))\nplt.scatter(linear_regression.deneyim, linear_regression.maas , s=75)\nplt.xlabel(\"deneyim\")\nplt.ylabel(\"maas\")\nplt.title(\"line fit or linear regression\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<p>\ny = maas <br>\nx = deneyim <br>\n<br>    \ny = b0 + b1*x <br>\nb0 = constant(bias) <br>\nb1 = coeff , b1 = y/x<br>    \n</p>","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"**MSE (Mean Squared Error):**\n\n$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n\nWhere:\n- $y_i$ = Actual value\n- $\\hat{y}_i$ = Predicted value\n- $n$ = Number of samples","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Linear regression model\nlinear_reg = LinearRegression()\n\nx = linear_regression.deneyim.values.reshape(-1,1)\ny = linear_regression.maas.values.reshape(-1,1)\n\nlinear_reg.fit(x,y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot data with fit line\n\ny_head = linear_reg.predict(x)\n\nplt.scatter(x,y)\nplt.plot(x,y_head,color = \"red\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find b0\nb0 = linear_reg.predict([[0]]) # manuel\nprint(\"b0: \",b0)\n\nb0 = linear_reg.intercept_ # library\nprint(\"b0: \",b0) # intercept","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find b1\nb1 = linear_reg.coef_\nprint(\"b1: \",b1) # slope","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<p>\ny = b0 + b1*x <br>\nmaas = b0 + b1*deneyim <br>\nmaas = 1663 + 1138*deneyim<br>     \n</p>","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Average salary for employees with 11 years of experience\n\nmaas_yeni = 1663 + 1138*11\nprint(\"11 yÄ±l deneyimli ortalama eleman maaÅŸÄ±: \",maas_yeni) # manuel\n\nprint(\"11 yÄ±l deneyimli ortalama eleman maaÅŸÄ±: \",linear_reg.predict([[11]])) # library","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<span style=\"color:red; font-weight:bold;\">There is no need to find b0 or b1 for predict</span>\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<a id = \"2\"></a>\n## Multiple Linear Regression","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import data\nmultiple_linear_regression = pd.read_csv(\"/kaggle/input/multiple-linear-regression-dataset/multiple_linear_regression_dataset.csv\", sep = \";\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiple_linear_regression.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiple_linear_regression.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot data\nplt.figure(figsize=(10, 6))\nplt.scatter(multiple_linear_regression['deneyim'], \n            multiple_linear_regression['maas'], \n            c=multiple_linear_regression['yas'], \n            cmap='viridis', \n            s=100, \n            alpha=0.6)\nplt.colorbar(label='YaÅŸ')\nplt.xlabel('Deneyim (YÄ±l)', fontsize=12)\nplt.ylabel('MaaÅŸ', fontsize=12)\nplt.title('Deneyim - MaaÅŸ Ä°liÅŸkisi (YaÅŸa GÃ¶re RenklendirilmiÅŸ)', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<p>\nsimple linear regression = y = b0 + b1*x <br>\nmultiple linear regression = y = b0 + b1*x1 + b2*x2 <br>\n<br>    \nmaas = dependent variable <br>\ndeneyim, yas = independent variable <br>   \n</p>","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Multiple Linear regression model\nx = multiple_linear_regression[['deneyim', 'yas']].values\ny = multiple_linear_regression.maas.values.reshape(-1,1)\n\nmultiple_linear_reg = LinearRegression()\nmultiple_linear_reg.fit(x,y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find b0\nprint(\"b0: \",multiple_linear_reg.intercept_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find b1,b2\nprint(\"b1,b2: \",multiple_linear_reg.coef_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict\nmultiple_linear_reg.predict(np.array([[10,35],[5,35]])) # Salary of 35-year-olds with 5 and 10 years of experience","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<a id = \"3\"></a>\n## Polynomial Lineer Regression","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import data\npolynomial_regression = pd.read_csv(\"/kaggle/input/polynomial-regression/polynomialregression.csv\", sep = \";\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"polynomial_regression.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"polynomial_regression.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"polynomial_regression.tail()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Linear regression model\nx = polynomial_regression.araba_fiyat.values.reshape(-1,1)\ny = polynomial_regression.araba_max_hiz.values.reshape(-1,1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot data\nplt.scatter(x,y)\nplt.ylabel(\"arabanÄ±n maksimum hÄ±zÄ±\")\nplt.xlabel(\"arabanÄ±n fiyatÄ±\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Linear regression model\npolynomial_reg = LinearRegression()\npolynomial_reg.fit(x,y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict\ny_head = polynomial_reg.predict(x)\nprint(y_head)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot data with fit line\nplt.scatter(x,y)\nplt.plot(x,y_head,color=\"red\")\nplt.xlabel(\"arabanÄ±n fiyatÄ±\")\nplt.ylabel(\"arabanÄ±n maksimum hÄ±zÄ±\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction 10.000 value car speed\npolynomial_reg.predict([[10000]]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<span style=\"color:red;\">Wrong analysis</span>","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"**Polynomial Linear Regression:**\n\n$$y = b_0 + b_1x + b_2x^2 + b_3x^3 + \\ldots + b_nx^n$$\n\nor in compact form:\n\n$$y = \\sum_{i=0}^{n} b_i x^i$$\n\nWhere:\n- $y$ = Predicted value\n- $b_0$ = Intercept\n- $b_1, b_2, \\ldots, b_n$ = Coefficients\n- $x$ = Independent variable\n- $n$ = Degree of polynomial\n\n---\n\n**Examples:**\n\n**2nd Degree (Quadratic):**\n$$y = b_0 + b_1x + b_2x^2$$\n\n**3rd Degree (Cubic):**\n$$y = b_0 + b_1x + b_2x^2 + b_3x^3$$","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Polynomial linear regression model\npolynomial_regression = PolynomialFeatures(degree = 2)\n\nx_polynomial = polynomial_regression.fit_transform(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate results\nx_polynomial","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Linear regression model\npolynomial_regressionn = LinearRegression()\n\npolynomial_regressionn.fit(x_polynomial,y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot data with fit curve \ny_headd = polynomial_regressionn.predict(x_polynomial)\n\nplt.scatter(x,y)\nplt.plot(x,y_headd,color=\"green\",label=\"Polynomial\")\nplt.plot(x,y_head,color=\"red\",label=\"Linear\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Higher degree equation fit curve  \npolynomial_regressionn = PolynomialFeatures(degree = 4)\nx_polynomiall = polynomial_regressionn.fit_transform(x)\npolynomial_regressionnn = LinearRegression()\npolynomial_regressionnn.fit(x_polynomiall,y)\ny_headdd = polynomial_regressionnn.predict(x_polynomiall)\n\nplt.scatter(x,y)\nplt.plot(x,y_headd,color=\"green\",label=\"Polynomial\")\nplt.plot(x,y_head,color=\"red\",label=\"Linear\")\nplt.plot(x,y_headdd,color=\"blue\",label=\"Higher degree polynomial\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<a id = \"4\"></a>\n## Decision Tree Regression","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import data\ntree_regression = pd.read_csv(\"/kaggle/input/decision-tree-regression-dataset/decisiontreeregressiondataset.csv\", sep = \";\",header=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tree_regression.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tree_regression.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Data\nplt.figure(figsize=(10, 6))\nplt.scatter(tree_regression[0], tree_regression[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<p>\n<strong>CART:</strong> Classification and Regression Tree<br>\n<strong>Variance:</strong> Measures the spread/disorder in target values (used in regression)<br>\n<strong>Terminal Leaf:</strong> Final node that makes predictions by averaging target values of samples in that node (also called Leaf)<br>\n<strong>Split:</strong> The point where a node divides into child nodes based on a feature threshold<br>\n<strong>Tree Model:</strong> Hierarchical structure that recursively splits data to minimize variance<br>\n<br>\n<strong>Example:</strong> If splitting reduces variance from 50 to 12.5, the Information Gain is 37.5\n</p>\n\n**Variance (for Regression):**\n$$Variance = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2$$\n\n**Variance Reduction (Information Gain):**\n$$Reduction = Variance_{parent} - \\left[\\frac{n_{left}}{n_{total}} \\times Variance_{left} + \\frac{n_{right}}{n_{total}} \\times Variance_{right}\\right]$$\n\nWhere:\n- $y_i$ = Each data point\n- $\\bar{y}$ = Mean value\n- $n$ = Number of samples\n- $n_{left}$, $n_{right}$ = Number of samples in left and right child nodes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision Tree Regression model\ntree_reg= DecisionTreeRegressor()\n\nx = tree_regression.iloc[:,0].values.reshape(-1,1)\ny = tree_regression.iloc[:,1].values.reshape(-1,1)\n\ntree_reg.fit(x,y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict\nx_ = np.arange(x.min(),x.max(),0.01).reshape(-1,1)\ny_head = tree_reg.predict(x_)\n\ntree_reg.predict([[6]])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot with Predict\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=x.flatten(),\n    y=y.flatten(),\n    mode='markers',\n    name='Actual Data',\n    marker=dict(color='orange', size=10)\n))\n\nfig.add_trace(go.Scatter(\n    x=x_.flatten(),\n    y=y_head.flatten(),\n    mode='lines',\n    name='Prediction',\n    line=dict(color='purple', width=3)\n))\n\nfig.update_layout(\n    title='Decision Tree Regression',\n    xaxis_title='Tribun Level',\n    yaxis_title='Price',\n    template='plotly_white',\n    width=900,\n    height=600\n)\n\nfig.show(config={'displayModeBar': False})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<a id = \"5\"></a>\n## Random Forest Regression","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import data\nrandom_forest_regression = pd.read_csv(\"/kaggle/input/random-forest-regression-dataset/randomforestregressiondataset.csv\", sep = \";\",header=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_forest_regression.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Data\nplt.figure(figsize=(10, 6))\nplt.scatter(random_forest_regression[0], random_forest_regression[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<p>\n<strong>Random Forest:</strong> Ensemble learning method that combines multiple decision trees<br>\n<strong>Ensemble Learning:</strong> Technique that creates multiple models and combines their predictions for better accuracy<br>\n<strong>Bagging (Bootstrap Aggregating):</strong> Each tree is trained on a random sample (with replacement) from the original data<br>\n<strong>Sub-data:</strong> Random subset of the original dataset used to train each individual tree<br>\n<strong>n Trees:</strong> The forest consists of multiple independent decision trees (tree1, tree2, tree3, ..., tree n)<br>\n<strong>Averaging:</strong> Final prediction is made by averaging the predictions of all trees<br>\n<strong>Result:</strong> More stable and accurate predictions compared to a single decision tree<br>\n<br>\n<strong>Example:</strong> If tree1 predicts 100, tree2 predicts 110, tree3 predicts 105, the final result = (100+110+105)/3 = 105\n</p>\n\n**Random Forest Prediction:**\n$$\\hat{y} = \\frac{1}{n}\\sum_{i=1}^{n}Tree_i(x)$$\n\n**Why Random Forest is Better:**\n$$Variance_{single\\_tree} > Variance_{random\\_forest}$$\n\nWhere:\n- $\\hat{y}$ = Final prediction\n- $n$ = Number of trees in the forest\n- $Tree_i(x)$ = Prediction from i-th tree\n- Lower variance = More stable and reliable predictions\n- Each tree sees different random samples, reducing overfitting","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random Forest Regression model\nrandom_forest_reg= RandomForestRegressor(n_estimators = 100,random_state = 42)\n\nx = random_forest_regression.iloc[:,0].values.reshape(-1,1)\ny = random_forest_regression.iloc[:,1].values.ravel()\n\nrandom_forest_reg.fit(x,y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict\nx_ = np.arange(x.min(),x.max(),0.01).reshape(-1,1)\ny_head = random_forest_reg.predict(x_)\n\nprint(\"7.8 seviyesinde fiyatÄ±n ne kadar olduÄŸu\",random_forest_reg.predict([[7.8]]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot with Predict\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=x.flatten(),\n    y=y.flatten(),\n    mode='markers',\n    name='Actual Data',\n    marker=dict(color='orange', size=10)\n))\n\nfig.add_trace(go.Scatter(\n    x=x_.flatten(),\n    y=y_head.flatten(),\n    mode='lines',\n    name='Prediction',\n    line=dict(color='purple', width=3)\n))\n\nfig.update_layout(\n    title='Decision Tree Regression',\n    xaxis_title='Tribun Level',\n    yaxis_title='Price',\n    template='plotly_white',\n    width=900,\n    height=600\n)\n\nfig.show(config={'displayModeBar': False})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<a id = \"6\"></a>\n## Evaluation Regression Models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"<p>\n<strong>Residual (Error):</strong> The difference between actual value and predicted value<br>\n<strong>Formula:</strong> residual = y - y_head<br>\n<strong>Square Residual:</strong> Squared error - eliminates negative values and penalizes larger errors more heavily<br>\n<strong>Formula:</strong> square residual = (residual)^2<br>\n<strong>SSR (Sum of Squared Residuals):</strong> Total prediction error of the model - lower is better<br>\n<strong>Formula:</strong> SSR = sum((y - y_head)^2)<br>\n<br>\n<strong>y_avg:</strong> Average of all actual values (used as baseline for comparison)<br>\n<strong>SST (Sum of Squares Total):</strong> Total variance in the data - fixed value for the dataset<br>\n<strong>Formula:</strong> SST = sum((y - y_avg)^2)<br>\n<br>\n<strong>Visual Explanation:</strong> In the graph, blue dots represent actual values, red line shows model predictions, and purple horizontal line indicates the average. Vertical distances represent residuals (errors).<br>\n<br>\n<strong>Example:</strong> If y=18000 and y_head=17500 â†’ residual = 500 â†’ square residual = 250,000\n</p>\n\n**RÂ² (R-Squared) - Coefficient of Determination:**\n$$R^2 = 1 - \\frac{SSR}{SST}$$\n\n**Interpretation:**\n- $R^2 = 1$ â†’ Perfect prediction (all points on the line, SSR = 0)\n- $R^2 = 0.8$ â†’ Model explains 80% of the variance in the data\n- $R^2 = 0$ â†’ Model is no better than using the mean (SSR = SST)\n- $R^2 < 0$ â†’ Model is worse than using the mean (very poor model!)\n- **The closer RÂ² value is to 1, the better the model performance**\n\n**Where:**\n- $SSR$ = Model's prediction error (want this LOW)\n- $SST$ = Total variance in data (fixed for dataset)\n- Lower SSR â†’ Higher $R^2$ â†’ Better model\n- $R^2$ shows percentage of variance explained by the model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict with Random Forest Regression\nrandom_forest_reg= RandomForestRegressor(n_estimators = 100,random_state = 42)\n\nx = random_forest_regression.iloc[:,0].values.reshape(-1,1)\ny = random_forest_regression.iloc[:,1].values.ravel()\n\nrandom_forest_reg.fit(x,y)\n\ny_head = random_forest_reg.predict(x)\n\nprint(f\"RÂ² Score: {r2_score(y,y_head)}\")\nprint(f\"Modelin aÃ§Ä±kladÄ±ÄŸÄ± varyans: %{(r2_score(y,y_head)*100)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict with Linear Regression\nlinear_reg = LinearRegression()\n\nx = linear_regression.deneyim.values.reshape(-1,1)\ny = linear_regression.maas.values.reshape(-1,1)\n\nlinear_reg.fit(x,y)\n\ny_head = linear_reg.predict(x)\n\nprint(f\"RÂ² Score: {r2_score(y,y_head)}\")\nprint(f\"Modelin aÃ§Ä±kladÄ±ÄŸÄ± varyans: %{(r2_score(y,y_head)*100)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict with Random Forest Regression\nrandom_forest_reg= RandomForestRegressor(n_estimators = 100,random_state = 42)\n\nx = random_forest_regression.iloc[:,0].values.reshape(-1,1)\ny = random_forest_regression.iloc[:,1].values.ravel()\n\nrandom_forest_reg.fit(x,y)\n\n# x_ = np.arange(x.min(),x.max(),0.01).reshape(-1,1)\ny_head = random_forest_reg.predict(x)\n\nprint(f\"RÂ² Score: {r2_score(y,y_head)}\")\nprint(f\"Modelin aÃ§Ä±kladÄ±ÄŸÄ± varyans: %{(r2_score(y,y_head)*100)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ”— References\n- Udemy Course: MACHINE LEARNING by DATAI TEAM","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}